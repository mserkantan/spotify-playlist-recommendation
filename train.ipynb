{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOgOmPcPxigCh4K6KS4JqVS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mserkantan/spotify-playlist-recommendation/blob/trials/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNaw6UIGWLzV",
        "outputId": "18c58f08-3b2d-428c-eaa5-e9417f4ab590"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        " \n",
        "import os, sys, time\n",
        "from tqdm import tqdm \n",
        "import json\n",
        " \n",
        "# sklearn libraries\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        " \n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model, load_model, save_model\n",
        "from keras.layers.core import Dense, Lambda, Activation\n",
        "from keras.layers import Embedding, Input, Dense, merge, Reshape, Flatten, Multiply, Concatenate, Add, Average\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        " \n",
        "# configure\n",
        "%matplotlib inline  \n",
        "style.use('fivethirtyeight')\n",
        "sns.set(style='whitegrid',color_codes=True)\n",
        " \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        " \n",
        "sys.path.append('/content/drive/MyDrive/spotify-playlist-recommendation')\n",
        "from utils import get_all_songs_df, get_negative_samples, get_negative_samples_test, get_playlists_df, get_test_samples, print_top_k_acc"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAnN1vqskfgP"
      },
      "source": [
        "class NCFDriver:\n",
        "  def __init__(self, params, load_dataset=False):\n",
        "    '''\n",
        "      Include all the parameters that can be tweaked\n",
        "      @args: NO, for differentiaton purposes\n",
        "      @args: file_count, total chunk of playlists to be included 10 corresponds to 10 * 1000 = 10.000\n",
        "      @args: threshold, mininum number of tracks for playlists to be accepted as valid input\n",
        "      @args: input_vector, the data for creating embeddings for example, playlist_uri, track_uri, album_uri, artist_uri\n",
        "      @args: neg_count, amount of wrong samples to prevent data imbalance\n",
        "      @args: test_count, how many samples will be separated from each dataset for evaluation\n",
        "      @args: embed_out, embedded vector output sizes default 16 for all.\n",
        "      @args: conc_method, how to fuse embeddings (\"element-wise\" is default)\n",
        "      @args: layers (list), layer input outputs that will be used in MLP. \n",
        "      @args: activation, which transfer function will be used for non-linearitization (default is sigmoid)\n",
        "      @args: optimizer, optimization algorithm (adam is default)\n",
        "      @args: lr, learning rate (1e-3 is default)\n",
        "      @args: loss_fcn, loss function (BCE is default) \n",
        "      @args: val_split, train_validation split (0.15 is default)\n",
        "      @args: batch_size, (default is 256)\n",
        "      @args: epoch, number of epochs for training (3 is default)\n",
        "    '''    \n",
        "    #assert len(params.keys()) == 16, \"Expected Dictionary Size is 16, given {}\".format(len(params.keys())) #FIXME: Ignore that for test purposes! \n",
        "\n",
        "    if \"NO\" in params.keys():\n",
        "      self.NO = params[\"NO\"]\n",
        "    else: \n",
        "      raise AssertionError('The key \"NO\" is expected!')\n",
        "\n",
        "    self.FILE_COUNT   = params[\"file_count\"]    if \"file_count\"   in params.keys() else 10\n",
        "    self.THRESHOLD    = params[\"threshold\"]     if \"threshold\"    in params.keys() else 50\n",
        "    self.INPUT_VECTOR = params[\"input_vector\"]  if \"input_vector\" in params.keys() else [\"pid\", \"track_uri\", \"artist_uri\"]\n",
        "    self.NEG_COUNT    = params[\"neg_count\"]     if \"neg_count\"    in params.keys() else 20\n",
        "    self.TEST_COUNT   = params[\"test_count\"]    if \"test_count\"   in params.keys() else 1\n",
        "    self.EMBED_OUT    = params[\"embed_out\"]     if \"embed_out\"    in params.keys() else [16 for i in self.INPUT_VECTOR]\n",
        "    self.CONC_METHOD  = params[\"conc_method\"]   if \"conc_method\"  in params.keys() else \"Multiply\"\n",
        "    self.LAYERS       = params[\"layers\"]        if \"layers\"       in params.keys() else [1]\n",
        "    self.ACTIVATION   = params[\"activation\"]    if \"activation\"   in params.keys() else \"sigmoid\"\n",
        "    self.OPTIMIZER    = params[\"optimizer\"]     if \"optimizer\"    in params.keys() else \"adam\"\n",
        "    self.LR           = params[\"lr\"]            if \"lr\"           in params.keys() else 1e-3\n",
        "    self.LOSS_FCN     = params[\"loss_fcn\"]      if \"loss_fcn\"     in params.keys() else \"binary_crossentropy\"\n",
        "    self.VAL_SPLIT    = params[\"val_split\"]     if \"val_split\"    in params.keys() else 0.15\n",
        "    self.BATCH_SIZE   = params[\"batch_size\"]    if \"batch_size\"   in params.keys() else 256\n",
        "    self.EPOCH        = params[\"epoch\"]         if \"epoch\"        in params.keys() else 3\n",
        "\n",
        "\n",
        "    assert \"track_uri\" in self.INPUT_VECTOR, 'Expected \"track_uri\" in input_vector, given features: {}'.format(self.INPUT_VECTOR)\n",
        "\n",
        "    assert \"pid\" in self.INPUT_VECTOR, 'Expected \"pid\" in input_vector, given features: {}'.format(self.INPUT_VECTOR)\n",
        "\n",
        "\n",
        "    self.encoded_labels = {name: LabelEncoder() for name in self.INPUT_VECTOR}\n",
        "    self.ids = []\n",
        "    self.names = []\n",
        "\n",
        "\n",
        "    self._MERGE_OPTIONS = {\"Multiply\":    Multiply, \n",
        "                           \"Concatenate\": Concatenate, \n",
        "                           \"Add\":         Add, \n",
        "                           \"Average\":     Average,\n",
        "                           }\n",
        "\n",
        "    print(\"The Merge Option is: {} \".format(self._MERGE_OPTIONS[self.CONC_METHOD]))\n",
        "    print(\"Embedded Vector Sizes: {}\".format(self.EMBED_OUT))\n",
        "\n",
        "    if not load_dataset:\n",
        "      self.load_data()\n",
        "    else:\n",
        "      self.training_df = pd.read_csv('{}_training_df.csv'.format(self.NO))\n",
        "      self.test_df = pd.read_csv('{}_test_df.csv'.format(self.NO))\n",
        "\n",
        "    self.create_embeddings()\n",
        "\n",
        "    # Choose the self.CONCAT_METHOD arg with the following dictionary keys\n",
        "\n",
        "    self.train()\n",
        "    self.predict()\n",
        "\n",
        "  def load_data(self):\n",
        "\n",
        "    def get_id_name(name):\n",
        "      if name == \"pid\":\n",
        "        self.names.append(\"playlist\")\n",
        "        self.ids.append(\"playlist_id\")\n",
        "        return \"playlist_id\"\n",
        "      elif name == \"track_uri\":\n",
        "        self.names.append(\"track\")\n",
        "        self.ids.append(\"track_id\")\n",
        "        return \"track_id\"\n",
        "      elif name == \"album_uri\":\n",
        "        self.names.append(\"album\")\n",
        "        self.ids.append(\"album_id\")\n",
        "        return \"album_id\"\n",
        "      elif name == \"artist_uri\":\n",
        "        self.names.append(\"artist\")\n",
        "        self.ids.append(\"artist_id\")\n",
        "        return \"artist_id\"\n",
        "      else:\n",
        "        raise AssertionError(\"An error Occured argument 'input_vector' has invalid column names, {}\".format(name))\n",
        "\n",
        "    self.playlists_df = get_playlists_df(number_of_files=self.FILE_COUNT)\n",
        "    self.playlists_df = self.playlists_df[self.playlists_df.num_tracks > self.THRESHOLD]\n",
        "    self.all_songs_df = get_all_songs_df(self.playlists_df)\n",
        "    self.training_df = self.all_songs_df[self.INPUT_VECTOR]\n",
        "    self.training_df['interaction'] = 1\n",
        "    self.all_unique_songs = self.training_df['track_uri'].unique()\n",
        "\n",
        "    print(\"Although we have {} tracks in all playlists we include, there are {} unique tracks.\\n\".format(self.training_df.shape[0], len(self.all_unique_songs)))\n",
        "\n",
        "\n",
        "    self.neg_samples_df = get_negative_samples(self.training_df, self.all_unique_songs, number_of_neg_sample=self.NEG_COUNT, inp=self.INPUT_VECTOR[2:])\n",
        "    self.training_df = pd.concat([self.training_df, self.neg_samples_df])\n",
        "    self.training_df.sort_values(['pid', 'interaction'], ascending=[True, False], inplace=True)\n",
        "    self.training_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    for inp in self.INPUT_VECTOR:\n",
        "      col = getattr(self.training_df, inp)\n",
        "      self.encoded_labels[inp].fit(col)\n",
        "      id_name = get_id_name(inp) #FIXME: A little shaky cant be trusted, but works for now.\n",
        "      self.training_df[id_name] = self.encoded_labels[inp].transform(col)\n",
        "\n",
        "    self.test_sample_indices = get_test_samples(self.training_df, number_of_test_sample=self.TEST_COUNT)\n",
        "    self.test_df = self.training_df.iloc[self.test_sample_indices,:]\n",
        "    self.test_df.reset_index(drop=True, inplace=True)\n",
        "    self.training_df = self.training_df.drop(self.test_sample_indices)\n",
        "    self.training_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    self.training_df.to_csv('{}_training_df.csv'.format(self.NO), index=False) \n",
        "    self.test_df.to_csv('{}_test_df.csv'.format(self.NO), index=False) \n",
        "\n",
        "    self.training_df = pd.read_csv('{}_training_df.csv'.format(self.NO))\n",
        "    self.test_df = pd.read_csv('{}_test_df.csv'.format(self.NO))\n",
        "\n",
        "    self.train_test_df = pd.concat([self.training_df, self.test_df])\n",
        "    self.train_test_df = self.train_test_df[self.train_test_df.interaction == 1]\n",
        "    self.train_test_df.sort_values('pid', inplace=True)\n",
        "    self.train_test_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "    all_unique_track_ids = self.train_test_df.track_id.unique()\n",
        "    self.neg_samples_for_test = get_negative_samples_test(self.train_test_df, all_unique_track_ids, number_of_neg_sample=99, inp=self.ids[2:]) #buraya embed liste eklenecek\n",
        "\n",
        "\n",
        "  def create_embeddings(self):\n",
        "    self.number_of = {id.split(\"_\")[0]: self.training_df[id].unique().max() + 1 for id in self.ids}\n",
        "\n",
        "    self.emb_vec_size = {name: self.EMBED_OUT[x] for x, name in enumerate(self.names)}\n",
        "\n",
        "    self.model_input = {name: Input(shape=(1,), dtype='int32', name = '{}_input'.format(name)) for name in self.names}\n",
        "\n",
        "    self.embedding_of = {name: Embedding(input_dim = self.number_of[name], \n",
        "                                  output_dim = self.emb_vec_size[name], \n",
        "                                  name = '{}_embedding'.format(name),\n",
        "                                  input_length=1) for name in self.names}\n",
        "\n",
        "    self.embedded_vec_of = {name: Flatten()(self.embedding_of[name](self.model_input[name])) for name in self.names}\n",
        "\n",
        "  def train(self):\n",
        "    # Element-wise product of playlist and track embeddings \n",
        "\n",
        "    self.predict_vector = self._MERGE_OPTIONS[self.CONC_METHOD]()(self.embedded_vec_of.values()) #FIXME: Add more concatenation option!\n",
        "\n",
        "    self.prediction = Dense(1, activation=self.ACTIVATION, name = 'prediction')(self.predict_vector) #FIXME: To be able to add more layers by arguments!\n",
        "\n",
        "    self.model = Model(inputs=self.model_input.values(), outputs=self.prediction)\n",
        "\n",
        "    self.model.compile(optimizer=Adam(lr=self.LR), loss=self.LOSS_FCN, metrics=['accuracy']) #FIXME: Change for the optimizer as well!\n",
        "\n",
        "    vals = [self.training_df[id].values for id in self.ids]\n",
        "    self.hist = self.model.fit(vals,\n",
        "                      self.training_df.interaction.values,\n",
        "                      validation_split=self.VAL_SPLIT, \n",
        "                      batch_size=self.BATCH_SIZE, \n",
        "                      epochs=self.EPOCH, \n",
        "                      shuffle=True)\n",
        "  def predict(self):\n",
        "    test_vals = [self.test_df[id].values for id in self.ids]\n",
        "    test_len = len(test_vals[0])\n",
        "\n",
        "\n",
        "    test_scores = []\n",
        "    for i in tqdm(range(test_len), position=0, leave=True):\n",
        "      score = self.model.predict([np.reshape([test_vals[id][i]], (-1,1)) for id in range(len(self.ids))])\n",
        "      test_scores.append(score[0][0])\n",
        "\n",
        "    print(\"\\n{}%\\n\".format(round(np.array(test_scores).mean()*100,2)))\n",
        "\n",
        "    test_df_2 = pd.concat([self.neg_samples_for_test, self.test_df[self.ids + ['interaction']]])\n",
        "    test_df_2.sort_values(['playlist_id', 'interaction'], ascending=[True, False], inplace=True)\n",
        "    test_df_2.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    all_scores = []\n",
        "    \n",
        "    unique_pid_list = test_df_2.playlist_id.unique()\n",
        "    for pid in tqdm(range(len(unique_pid_list)), position=0, leave=True):\n",
        "\n",
        "      pid_scores = []\n",
        "      ids_of = {name: test_df_2[test_df_2.playlist_id == pid][self.ids[id]].values for id, name in enumerate(self.names)}\n",
        "\n",
        "      for id in range(len(ids_of[\"track\"])): #from tid to id (generalizing to n dimensional matrix factorization)\n",
        "          arr = [np.reshape([pid], (-1,1))]\n",
        "          for name in self.names:\n",
        "            if name != \"playlist\":\n",
        "              arr.append(np.reshape([ids_of[name][id]], (-1,1)))\n",
        "\n",
        "          score = self.model.predict(arr)\n",
        "          pid_scores.append(score[0][0])\n",
        "        \n",
        "      all_scores.append(pid_scores)\n",
        "\n",
        "\n",
        "    np.savetxt('{}_all_scores.txt'.format(self.NO), all_scores)\n",
        "\n",
        "    #to load back\n",
        "    all_scores_loaded = np.loadtxt('{}_all_scores.txt'.format(self.NO))\n",
        "\n",
        "    uniuqe_pid_list = test_df_2.playlist_id.unique()\n",
        "\n",
        "    test_click_ranks = []\n",
        "\n",
        "    for pid in tqdm(range(len(unique_pid_list)),position=0, leave=True):\n",
        "      pid_df = test_df_2[test_df_2.playlist_id == pid]\n",
        "      pid_df['pred_scores'] = all_scores_loaded[pid,:]\n",
        "      pid_df.sort_values('pred_scores', ascending=False, inplace=True)\n",
        "      pid_df.reset_index(drop=True, inplace=True)\n",
        "      test_click_ranks.append(pid_df[pid_df.interaction==1].index.values[0])\n",
        "\n",
        "    test_click_ranks = np.array(test_click_ranks)\n",
        "\n",
        "    test_click_ranks.mean()\n",
        "\n",
        "\n",
        "    print_top_k_acc(test_click_ranks,1)\n",
        "    print_top_k_acc(test_click_ranks,2)\n",
        "    print_top_k_acc(test_click_ranks,5)\n",
        "    print_top_k_acc(test_click_ranks,10)\n",
        "    print_top_k_acc(test_click_ranks,20)\n",
        "    "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8REUvO0KsFPS",
        "outputId": "4ca592f5-217f-46bc-8894-74095dd6940f"
      },
      "source": [
        "params = {\"NO\": 1,\n",
        "          \"input_vector\": [\"pid\", \"track_uri\", \"album_uri\"]\n",
        "          }\n",
        "nfc = NCFDriver(params=params, load_dataset=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4907 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Although we have 526065 tracks in all playlists we include, there are 146399 unique tracks.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4907/4907 [20:38<00:00,  3.96it/s]\n",
            "100%|██████████| 4907/4907 [00:15<00:00, 323.63it/s]\n",
            "100%|██████████| 4907/4907 [08:03<00:00, 10.15it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "2057/2057 [==============================] - 61s 29ms/step - loss: 0.5812 - accuracy: 0.8411 - val_loss: 0.4528 - val_accuracy: 0.8364\n",
            "Epoch 2/3\n",
            "2057/2057 [==============================] - 59s 29ms/step - loss: 0.3799 - accuracy: 0.8422 - val_loss: 0.4833 - val_accuracy: 0.8364\n",
            "Epoch 3/3\n",
            "2057/2057 [==============================] - 58s 28ms/step - loss: 0.2664 - accuracy: 0.8479 - val_loss: 0.5744 - val_accuracy: 0.8228\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4907/4907 [03:54<00:00, 20.96it/s]\n",
            "  0%|          | 0/4907 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "84.3%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4907/4907 [6:38:30<00:00,  4.87s/it]\n",
            "100%|██████████| 4907/4907 [00:13<00:00, 354.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Top-1 accuracy: 0.6236\n",
            "Top-2 accuracy: 0.6568\n",
            "Top-5 accuracy: 0.6943\n",
            "Top-10 accuracy: 0.7188\n",
            "Top-20 accuracy: 0.7491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyVBuDNZtzSU"
      },
      "source": [
        "params = {\"NO\": 5,\n",
        "          \"conc_method\": \"Multiply\",  \n",
        "          \"input_vector\": [\"pid\", \"track_uri\", \"album_uri\"]\n",
        "          }\n",
        "nfc = NCFDriver(params=params, load_dataset=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrkkPAamQJKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe69392-2fca-434c-c6d5-b056a3615edf"
      },
      "source": [
        "params = {\"NO\": 3,\n",
        "          \"conc_method\": \"Concatenate\", \n",
        "          \"input_vector\": [\"pid\", \"track_uri\", \"album_uri\"]\n",
        "          }\n",
        "nfc = NCFDriver(params=params, load_dataset=False)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Merge Option is: <class 'tensorflow.python.keras.layers.merge.Concatenate'> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4907 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Although we have 526065 tracks in all playlists we include, there are 146399 unique tracks.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4907/4907 [10:11<00:00,  8.03it/s]\n",
            "100%|██████████| 4907/4907 [00:12<00:00, 393.39it/s]\n",
            "100%|██████████| 4907/4907 [05:08<00:00, 15.92it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "2057/2057 [==============================] - 65s 31ms/step - loss: 0.4419 - accuracy: 0.8258 - val_loss: 0.3640 - val_accuracy: 0.7770\n",
            "Epoch 2/3\n",
            "2057/2057 [==============================] - 65s 32ms/step - loss: 0.2171 - accuracy: 0.9074 - val_loss: 0.4617 - val_accuracy: 0.7623\n",
            "Epoch 3/3\n",
            "2057/2057 [==============================] - 63s 31ms/step - loss: 0.1554 - accuracy: 0.9395 - val_loss: 0.6042 - val_accuracy: 0.7574\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4907/4907 [03:01<00:00, 27.08it/s]\n",
            "  0%|          | 0/4907 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "87.27%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4907/4907 [4:44:41<00:00,  3.48s/it]\n",
            "100%|██████████| 4907/4907 [00:11<00:00, 443.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Top-1 accuracy: 0.2517\n",
            "Top-2 accuracy: 0.3711\n",
            "Top-5 accuracy: 0.5186\n",
            "Top-10 accuracy: 0.6226\n",
            "Top-20 accuracy: 0.7178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoG78Cygmulz",
        "outputId": "0cd5a1a3-abcd-4f48-93b1-0a7023bc1feb"
      },
      "source": [
        "params = {\"NO\": 5,\n",
        "          \"conc_method\": \"Multiply\",\n",
        "          \"embed_out\": [32, 32, 32],  \n",
        "          \"input_vector\": [\"pid\", \"track_uri\", \"album_uri\"]\n",
        "          }\n",
        "nfc = NCFDriver(params=params, load_dataset=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Merge Option is: <class 'tensorflow.python.keras.layers.merge.Multiply'> \n",
            "Embedded Vector Sizes: [32, 32, 32]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4907 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Although we have 526065 tracks in all playlists we include, there are 146399 unique tracks.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 19%|█▊        | 915/4907 [02:13<08:39,  7.68it/s]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}